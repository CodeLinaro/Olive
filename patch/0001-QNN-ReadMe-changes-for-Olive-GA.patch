From d8424dae508802aa1232ff2bd652c45e4e5dd5b2 Mon Sep 17 00:00:00 2001
From: Ronak Mahawar <rmahawar@qti.qualcomm.com>
Date: Fri, 19 Sep 2025 17:44:30 -0700
Subject: [PATCH] QNN ReadMe changes for Olive GA

---
 examples/deepseek/README.md                   |  4 +-
 ...pSeek_R1_Distill_Qwen_1.5B_qnn_config.json | 95 ++++++++++++++++++
 examples/llama3/README.md                     | 12 ++-
 .../qnn/llama3.1_8b_instruct_qnn_config.json  | 97 +++++++++++++++++++
 .../qnn/llama3.2_1b_instruct_qnn_config.json  | 95 ++++++++++++++++++
 examples/phi3_5/README.md                     | 11 ++-
 examples/phi4/README.md                       |  5 +
 examples/qwen2_5/README.md                    | 13 ++-
 .../qnn/qwen_2.5_7b_instruct_qnn_config.json  | 97 +++++++++++++++++++
 9 files changed, 417 insertions(+), 12 deletions(-)
 create mode 100644 examples/deepseek/qnn/DeepSeek_R1_Distill_Qwen_1.5B_qnn_config.json
 create mode 100644 examples/llama3/qnn/llama3.1_8b_instruct_qnn_config.json
 create mode 100644 examples/llama3/qnn/llama3.2_1b_instruct_qnn_config.json
 create mode 100644 examples/qwen2_5/qnn/qwen_2.5_7b_instruct_qnn_config.json

diff --git a/examples/deepseek/README.md b/examples/deepseek/README.md
index 76a62eec..2d545476 100644
--- a/examples/deepseek/README.md
+++ b/examples/deepseek/README.md
@@ -7,7 +7,7 @@ Sample use cases of Olive to optimize a [DeepSeek R1 Distill](https://huggingfac
   - Run the workflow with `olive run --config qdq_config.json -m deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B -o models/deepseek-r1-qdq`.
 - [AMD NPU: Optimization and Quantization with for VitisAI](../phi3_5/README.md):
   - Run the workflow with `olive run --config qdq_config_vitis_ai.json -m deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B -o models/deepseek-r1-vai`.
-- [PTQ + AOT Compilation for Qualcomm NPUs using QNN EP](../phi3_5/README.md):
-  - Run the workflow with `olive run --config qnn_config.json -m deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B -o models/deepseek-r1-qnn`.
+- [QUALCOMM NPU: PTQ + AOT Compilation using QNN EP](../phi3_5/README.md):
+  - Run the workflow with `olive run --config qnn/DeepSeek_R1_Distill_Qwen_1.5B_qnn_config.json`.
   - Run the inference with `python app.py -m models/deepseek-r1-qnn -c "<｜User｜>{input}<｜Assistant｜><think>"`.
 - [PTQ + AWQ ONNX OVIR Encapsulated 4-bit weight compression using Optimum OpenVINO](./openvino/)
diff --git a/examples/deepseek/qnn/DeepSeek_R1_Distill_Qwen_1.5B_qnn_config.json b/examples/deepseek/qnn/DeepSeek_R1_Distill_Qwen_1.5B_qnn_config.json
new file mode 100644
index 00000000..e7be1710
--- /dev/null
+++ b/examples/deepseek/qnn/DeepSeek_R1_Distill_Qwen_1.5B_qnn_config.json
@@ -0,0 +1,95 @@
+{
+    "input_model": { "type": "HfModel", "model_path": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" },
+    "systems": {
+        "qnn_system": {
+            "type": "PythonEnvironment",
+            "python_environment_path": "/path/to/qnn/env/bin",
+            "accelerators": [ { "execution_providers": [ "QNNExecutionProvider" ] } ]
+        }
+    },
+    "data_configs": [
+        {
+            "name": "wikitext2_train_joined",
+            "type": "HuggingfaceContainer",
+            "load_dataset_config": { "data_name": "wikitext", "subset": "wikitext-2-raw-v1", "split": "train" },
+            "pre_process_data_config": {
+                "strategy": "join",
+                "add_special_tokens": false,
+                "max_seq_len": 4096,
+                "max_samples": 128
+            }
+        },
+        {
+            "name": "wikitext2_train_act",
+            "type": "HuggingfaceContainer",
+            "load_dataset_config": { "data_name": "wikitext", "subset": "wikitext-2-raw-v1", "split": "train" },
+            "pre_process_data_config": {
+                "strategy": "line-by-line",
+                "add_special_tokens": true,
+                "max_samples": 256,
+                "max_seq_len": 4096
+            }
+        }
+    ],
+    "passes": {
+        "q": { "type": "QuaRot" },
+        "g": {
+            "type": "GptqQuantizer",
+            "sym": true,
+            "group_size": -1,
+            "desc_act": true,
+            "data_config": "wikitext2_train_joined"
+        },
+        "cs": { "type": "CaptureSplitInfo", "num_splits": 4, "unique_embeds_lm_head_splits": true },
+        "mb": {
+            "type": "ModelBuilder",
+            "precision": "int4",
+            "int4_block_size": 32,
+            "int4_accuracy_level": 4,
+            "int4_op_types_to_quantize": [ "MatMul", "Gather" ]
+        },
+        "mq": {
+            "type": "MatMulNBitsToQDQ",
+            "use_int4": true,
+            "add_zero_point": true,
+            "nodes_to_exclude": [ "/lm_head/MatMul_Q4" ],
+            "save_as_external_data": true
+        },
+        "gs": {
+            "type": "GraphSurgeries",
+            "surgeries": [
+                { "surgeon": "RemoveRopeMultiCache" },
+                { "surgeon": "AttentionMaskToSequenceLengths" },
+                { "surgeon": "SimplifiedLayerNormToL2Norm" }
+            ],
+            "save_as_external_data": true
+        },
+        "sq": {
+            "type": "OnnxStaticQuantization",
+            "data_config": "wikitext2_train_act",
+            "activation_type": "uint16",
+            "precision": "uint8",
+            "calibration_providers": [ "CUDAExecutionProvider" ],
+            "quant_preprocess": true,
+            "op_types_to_exclude": [ "GatherBlockQuantized", "GroupQueryAttention", "MatMulNBits" ],
+            "save_as_external_data": true
+        },
+        "sp": { "type": "SplitModel" },
+        "st": { "type": "StaticLLM", "batch_size": 1, "context_length": 64 },
+        "cb": {
+            "type": "EPContextBinaryGenerator",
+            "provider_options": {
+                "htp_performance_mode": "burst",
+                "htp_graph_finalization_optimization_mode": "3",
+                "soc_model": "60"
+            },
+            "weight_sharing": true
+        },
+        "cp": { "type": "ComposeOnnxModels" }
+    },
+    "target": "qnn_system",
+    "log_severity_level": 1,
+    "output_dir": "models/deepseek-r1-qnn",
+    "cache_dir": "cache",
+    "no_artifacts": true
+}
diff --git a/examples/llama3/README.md b/examples/llama3/README.md
index 0ea5122c..1acfc750 100644
--- a/examples/llama3/README.md
+++ b/examples/llama3/README.md
@@ -7,9 +7,8 @@ Sample use cases of Olive to optimize [meta-llama/Llama-3.2-1B-Instruct](https:/
   - Run the workflow with `olive run --config qdq_config.json -m meta-llama/Llama-3.2-1B-Instruct -o models/llama3-qdq`.
 - [AMD NPU: Optimization and Quantization with for VitisAI](../phi3_5/README.md):
   - Run the workflow with `olive run --config qdq_config_vitis_ai.json -m meta-llama/Llama-3.2-1B-Instruct -o models/llama3-vai`.
-- [PTQ + AOT Compilation for Qualcomm NPUs using QNN EP](../phi3_5/README.md):
-  - Run the workflow with `olive run --config qnn_config.json -m meta-llama/Llama-3.2-1B-Instruct -o models/llama3-qnn`.
-  - Run the inference with `python app.py -m models/llama3-qnn -c "<|start_header_id|>user<|end_header_id|>\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>"`.
+- [QUALCOMM NPU: PTQ + AOT Compilation using QNN EP](../phi3_5/README.md):
+  - Refer to the Qualcomm NPU section below.
 - [PTQ + AWQ ONNX OVIR Encapsulated 4-bit weight compression using Optimum OpenVINO](./openvino/)
 
 **NOTE:**
@@ -22,6 +21,13 @@ huggingface-cli login
 
 - The quality of the quantized model is not guaranteed to be the same as the original model, especially for such a small model. Work is ongoing to improve the quality of the quantized model.
 
+## **Optimization and Quantization for QUALCOMM NPU**
+
+- [QUALCOMM NPU: PTQ + AOT Compilation using QNN EP](../phi3_5/README.md):
+  - Run the workflow with `olive run --config qnn/llama3.2_1b_instruct_qnn_config.json`.
+  - Run the inference with `python app.py -m models/llama_3.2_1b -c "<|start_header_id|>user<|end_header_id|>\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>"`.
+  - Config for llama 3.1 8b instruct model at qnn/llama3.1_8b_instruct_qnn_config.json
+  - Python app.py to be modified accordingly for llama 3.1 8b instruct.
 ## **Optimization and Quantization for AMD NPU**
 
 - [**AMD NPU**](./vitisai/): Instructions to run quantization and optimization for AMD NPU are in the in the [vitisai](./vitisai/) folder.
diff --git a/examples/llama3/qnn/llama3.1_8b_instruct_qnn_config.json b/examples/llama3/qnn/llama3.1_8b_instruct_qnn_config.json
new file mode 100644
index 00000000..e430fa93
--- /dev/null
+++ b/examples/llama3/qnn/llama3.1_8b_instruct_qnn_config.json
@@ -0,0 +1,97 @@
+{
+    "input_model": { "type": "HfModel", "model_path": "meta-llama/Llama-3.1-8B-Instruct" },
+    "systems": {
+        "qnn_system": {
+            "type": "PythonEnvironment",
+            "python_environment_path": "/path/to/qnn/env/bin",
+            "accelerators": [ { "execution_providers": [ "QNNExecutionProvider" ] } ]
+        }
+    },
+    "data_configs": [
+        {
+            "name": "wikitext2_train_joined",
+            "type": "HuggingfaceContainer",
+            "load_dataset_config": { "data_name": "wikitext", "subset": "wikitext-2-raw-v1", "split": "train" },
+            "pre_process_data_config": {
+                "strategy": "join",
+                "add_special_tokens": false,
+                "max_seq_len": 4096,
+                "max_samples": 128
+            }
+        },
+        {
+            "name": "wikitext2_train_act",
+            "type": "HuggingfaceContainer",
+            "load_dataset_config": { "data_name": "wikitext", "subset": "wikitext-2-raw-v1", "split": "train" },
+            "pre_process_data_config": {
+                "strategy": "line-by-line",
+                "add_special_tokens": true,
+                "max_samples": 256,
+                "max_seq_len": 4096
+            }
+        }
+    ],
+    "passes": {
+        "q": { "type": "QuaRot" },
+        "g": {
+            "type": "GptqModel",
+            "bits": 4,
+            "sym": true,
+            "group_size": -1,
+            "lm_head": false,
+            "device": "cuda",
+            "data_config": "wikitext2_train_joined"
+        },
+        "cs": { "type": "CaptureSplitInfo", "num_splits": 4, "unique_embeds_lm_head_splits": true },
+        "mb": {
+            "type": "ModelBuilder",
+            "precision": "int4",
+            "int4_block_size": 32,
+            "int4_accuracy_level": 4,
+            "int4_op_types_to_quantize": [ "MatMul", "Gather" ]
+        },
+        "mq": {
+            "type": "MatMulNBitsToQDQ",
+            "use_int4": true,
+            "add_zero_point": true,
+            "nodes_to_exclude": [ "/lm_head/MatMul_Q4" ],
+            "save_as_external_data": true
+        },
+        "gs": {
+            "type": "GraphSurgeries",
+            "surgeries": [
+                { "surgeon": "RemoveRopeMultiCache" },
+                { "surgeon": "AttentionMaskToSequenceLengths" },
+                { "surgeon": "SimplifiedLayerNormToL2Norm" }
+            ],
+            "save_as_external_data": true
+        },
+        "sq": {
+            "type": "OnnxStaticQuantization",
+            "data_config": "wikitext2_train_act",
+            "activation_type": "uint16",
+            "precision": "uint8",
+            "calibration_providers": [ "CUDAExecutionProvider" ],
+            "quant_preprocess": true,
+            "op_types_to_exclude": [ "GatherBlockQuantized", "GroupQueryAttention", "MatMulNBits" ],
+            "save_as_external_data": true
+        },
+        "sp": { "type": "SplitModel" },
+        "st": { "type": "StaticLLM", "batch_size": 1, "context_length": 64 },
+        "cb": {
+            "type": "EPContextBinaryGenerator",
+            "provider_options": {
+                "htp_performance_mode": "burst",
+                "htp_graph_finalization_optimization_mode": "3",
+                "soc_model": "60"
+            },
+            "weight_sharing": true
+        },
+        "cp": { "type": "ComposeOnnxModels" }
+    },
+    "target": "qnn_system",
+    "log_severity_level": 1,
+    "output_dir": "models/llama3.1_8b_instruct",
+    "cache_dir": "cache",
+    "no_artifacts": true
+}
diff --git a/examples/llama3/qnn/llama3.2_1b_instruct_qnn_config.json b/examples/llama3/qnn/llama3.2_1b_instruct_qnn_config.json
new file mode 100644
index 00000000..27f74537
--- /dev/null
+++ b/examples/llama3/qnn/llama3.2_1b_instruct_qnn_config.json
@@ -0,0 +1,95 @@
+{
+    "input_model": { "type": "HfModel", "model_path": "meta-llama/Llama-3.2-1B-Instruct" },
+    "systems": {
+        "qnn_system": {
+            "type": "PythonEnvironment",
+            "python_environment_path": "/path/to/qnn/env/bin",
+            "accelerators": [ { "execution_providers": [ "QNNExecutionProvider" ] } ]
+        }
+    },
+    "data_configs": [
+        {
+            "name": "wikitext2_train_joined",
+            "type": "HuggingfaceContainer",
+            "load_dataset_config": { "data_name": "wikitext", "subset": "wikitext-2-raw-v1", "split": "train" },
+            "pre_process_data_config": {
+                "strategy": "join",
+                "add_special_tokens": false,
+                "max_seq_len": 4096,
+                "max_samples": 128
+            }
+        },
+        {
+            "name": "wikitext2_train_act",
+            "type": "HuggingfaceContainer",
+            "load_dataset_config": { "data_name": "wikitext", "subset": "wikitext-2-raw-v1", "split": "train" },
+            "pre_process_data_config": {
+                "strategy": "line-by-line",
+                "add_special_tokens": true,
+                "max_samples": 256,
+                "max_seq_len": 4096
+            }
+        }
+    ],
+    "passes": {
+        "q": { "type": "QuaRot" },
+        "g": {
+            "type": "GptqQuantizer",
+            "sym": true,
+            "group_size": -1,
+            "desc_act": true,
+            "data_config": "wikitext2_train_joined"
+        },
+        "cs": { "type": "CaptureSplitInfo", "num_splits": 4, "unique_embeds_lm_head_splits": true },
+        "mb": {
+            "type": "ModelBuilder",
+            "precision": "int4",
+            "int4_block_size": 32,
+            "int4_accuracy_level": 4,
+            "int4_op_types_to_quantize": [ "MatMul", "Gather" ]
+        },
+        "mq": {
+            "type": "MatMulNBitsToQDQ",
+            "use_int4": true,
+            "add_zero_point": true,
+            "nodes_to_exclude": [ "/lm_head/MatMul_Q4" ],
+            "save_as_external_data": true
+        },
+        "gs": {
+            "type": "GraphSurgeries",
+            "surgeries": [
+                { "surgeon": "RemoveRopeMultiCache" },
+                { "surgeon": "AttentionMaskToSequenceLengths" },
+                { "surgeon": "SimplifiedLayerNormToL2Norm" }
+            ],
+            "save_as_external_data": true
+        },
+        "sq": {
+            "type": "OnnxStaticQuantization",
+            "data_config": "wikitext2_train_act",
+            "activation_type": "uint16",
+            "precision": "uint8",
+            "calibration_providers": [ "CUDAExecutionProvider" ],
+            "quant_preprocess": true,
+            "op_types_to_exclude": [ "GatherBlockQuantized", "GroupQueryAttention", "MatMulNBits" ],
+            "save_as_external_data": true
+        },
+        "sp": { "type": "SplitModel" },
+        "st": { "type": "StaticLLM", "batch_size": 1, "context_length": 64 },
+        "cb": {
+            "type": "EPContextBinaryGenerator",
+            "provider_options": {
+                "htp_performance_mode": "burst",
+                "htp_graph_finalization_optimization_mode": "3",
+                "soc_model": "60"
+            },
+            "weight_sharing": true
+        },
+        "cp": { "type": "ComposeOnnxModels" }
+    },
+    "target": "qnn_system",
+    "log_severity_level": 1,
+    "output_dir": "models/llama_3.2_1b",
+    "cache_dir": "cache",
+    "no_artifacts": true
+}
diff --git a/examples/phi3_5/README.md b/examples/phi3_5/README.md
index f04a0ec9..d8ab0299 100644
--- a/examples/phi3_5/README.md
+++ b/examples/phi3_5/README.md
@@ -9,7 +9,7 @@ This repository demonstrates the optimization of the [Microsoft Phi-3.5 Mini Ins
     - [**Usage**](#usage)
       - [**Quantization Python Environment Setup**](#quantization-python-environment-setup)
       - [**Run the Quantization Config**](#run-the-quantization-config)
-  - [**PTQ + AOT Compilation for Qualcomm NPUs using QNN EP**](#ptq--aot-compilation-for-qualcomm-npus-using-qnn-ep)
+  - [**QUALCOMM NPU: PTQ + AOT Compilation using QNN EP**](#ptq--aot-compilation-for-qualcomm-npus-using-qnn-ep)
     - [**Resource Optimization Strategy**](#resource-optimization-strategy)
     - [**Compilation for Qualcomm NPU Deployment**](#compilation-for-qualcomm-npu-deployment)
     - [**Usage**](#usage-1)
@@ -20,6 +20,7 @@ This repository demonstrates the optimization of the [Microsoft Phi-3.5 Mini Ins
       - [**Install Dependencies**](#install-dependencies)
       - [**Install Required Python Packages**](#install-required-python-packages)
       - [**Run Console-Based Chat Interface**](#run-console-based-chat-interface)
+  
   - [**PTQ + AOT Compilation for Intel® NPUs using Optimum Intel®**](#ptq--aot-compilation-for-intel-npus-using-optimum-intel)
   - [**AMD NPU: Optimization and Quantization with for VitisAI**](#optimization-and-quantization-for-amd-npu)
 
@@ -112,7 +113,7 @@ olive run --config qdq_config_vitis_ai.json
 
 ✅ Optimized model saved in: `models/phi3_5-vai/`
 
-## **PTQ + AOT Compilation for Qualcomm NPUs using QNN EP**
+## **QUALCOMM NPU: PTQ + AOT Compilation using QNN EP**
 
 This process extends the [**QDQ Model with 4-bit Weights & 16-bit Activations**](#qdq-model-with-4-bit-weights--16-bit-activations) by compiling it specifically for **Qualcomm NPUs** using the **QNN Execution Provider**.
 
@@ -154,7 +155,7 @@ Model compilation using QNN Execution Provider requires a Python environment wit
 ```bash
 # Install ONNX Runtime QNN
 pip install -r https://raw.githubusercontent.com/microsoft/onnxruntime/refs/heads/main/requirements.txt
-pip install --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple "onnxruntime-qnn==1.22.2" --no-deps
+pip install -U --pre --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple onnxruntime-qnn --no-deps
 ```
 
 Replace `/path/to/qnn/env/bin` in [qnn_config.json](qnn_config.json) with the path to the directory containing your QNN environment's Python executable. This path can be found by running the following command in the environment:
@@ -202,7 +203,9 @@ The optimized model can be used for inference using ONNX Runtime QNNExecutionPro
 Open ARM64 Native Tools Command Prompt for VS2022 and run the following commands:
 
 ```bash
-pip install "onnxruntime-qnn>=1.22.2" "onnxruntime-genai>=0.7.0"
+pip install -r https://raw.githubusercontent.com/microsoft/onnxruntime/refs/heads/main/requirements.txt
+pip install -U --pre --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple onnxruntime-qnn --no-deps
+pip install "onnxruntime-genai>=0.7.0"
 ```
 
 #### **Run Console-Based Chat Interface**
diff --git a/examples/phi4/README.md b/examples/phi4/README.md
index 6f659c78..ab732f31 100644
--- a/examples/phi4/README.md
+++ b/examples/phi4/README.md
@@ -10,6 +10,11 @@ This repository demonstrates the optimization of the following models using **po
 
 4. [Microsoft Phi-4-mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct)
 
+## **Optimization and Quantization for Qualcomm NPU**
+
+- [**QUALCOMM NPU**](../phi3_5/README.md): Instructions to run quantization and optimization for QUALCOMM NPU are in the in the [qnn](../phi3_5/README.md) folder.
+  - Config for phi4 mini instruct model present in qnn/phi4_mini_instruct_qnn_config.json
+
 ## **PTQ Weight Compression for Intel® NPUs/GPUs using Optimum Intel®**
 
 - [**Intel® NPU/GPU**](./openvino/): Optimization with Optimum Intel® on Intel® NPU/GPU to generate an ONNX OpenVINO IR Encapsulated Model instructions are in the [openvino](./openvino/) folder.
diff --git a/examples/qwen2_5/README.md b/examples/qwen2_5/README.md
index 0b1c01c5..4846ce48 100644
--- a/examples/qwen2_5/README.md
+++ b/examples/qwen2_5/README.md
@@ -4,13 +4,20 @@ Sample use cases of Olive to optimize a [Qwen/Qwen 2.5 1.5B Instruct](https://hu
 
 - [QDQ Model with 4-bit Weights & 16-bit Activations](../phi3_5/README.md):
   - Run the workflow with `olive run --config qdq_config.json -m Qwen/Qwen2.5-1.5B-Instruct -o models/qwen2_5-qdq`.
+- [QUALCOMM NPU: Optimization and Quantization using QNN EP](../phi3_5/README.md):
+  - Refer to the Qualcomm NPU section below.
 - [AMD NPU: Optimization and Quantization with for VitisAI](../phi3_5/README.md):
   - Run the workflow with `olive run --config qdq_config_vitis_ai.json -m Qwen/Qwen2.5-1.5B-Instruct -o models/qwen2_5-vai`.
-- [PTQ + AOT Compilation for Qualcomm NPUs using QNN EP](../phi3_5/README.md):
-  - Run the workflow with `olive run --config qnn_config.json -m Qwen/Qwen2.5-1.5B-Instruct -o models/qwen2_5-qnn`.
-  - Run the inference with `python app.py -m models/qwen2_5-qnn -c "<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n"`.
 - [PTQ + AWQ ONNX OVIR Encapsulated 4-bit weight compression using Intel® Optimum OpenVINO](./openvino/)
 
+## **Optimization and Quantization for QUALCOMM NPU**
+
+- [QUALCOMM NPU: PTQ + AOT Compilation using QNN EP](../phi3_5/README.md):
+  - Run the workflow with `olive run --config qnn/qwen_2.5_1.5b_instruct_qnn_config.json`.
+  - Run the inference with `python app.py -m models/qwen_2.5_1.5b_Instruct -c "<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n"`.
+  - Config for Qwen 2.5 7b instruct model at qnn/qwen_2.5_7b_instruct_qnn_config.json
+  - Python app.py to be modified accordingly for Qwen 2.5 7b instruct.
+
 ## **Optimization and Quantization for AMD NPU**
 
 - [**AMD NPU**](./vitisai/): Instructions to run quantization and optimization for AMD NPU are in the in the [vitisai](./vitisai/) folder.
diff --git a/examples/qwen2_5/qnn/qwen_2.5_7b_instruct_qnn_config.json b/examples/qwen2_5/qnn/qwen_2.5_7b_instruct_qnn_config.json
new file mode 100644
index 00000000..0a5e1f02
--- /dev/null
+++ b/examples/qwen2_5/qnn/qwen_2.5_7b_instruct_qnn_config.json
@@ -0,0 +1,97 @@
+{
+    "input_model": { "type": "HfModel", "model_path": "Qwen/Qwen2.5-7B-Instruct" },
+    "systems": {
+        "qnn_system": {
+            "type": "PythonEnvironment",
+            "python_environment_path": "/path/to/qnn/env/bin",
+            "accelerators": [ { "execution_providers": [ "QNNExecutionProvider" ] } ]
+        }
+    },
+    "data_configs": [
+        {
+            "name": "wikitext2_train_joined",
+            "type": "HuggingfaceContainer",
+            "load_dataset_config": { "data_name": "wikitext", "subset": "wikitext-2-raw-v1", "split": "train" },
+            "pre_process_data_config": {
+                "strategy": "join",
+                "add_special_tokens": false,
+                "max_seq_len": 4096,
+                "max_samples": 128
+            }
+        },
+        {
+            "name": "wikitext2_train_act",
+            "type": "HuggingfaceContainer",
+            "load_dataset_config": { "data_name": "wikitext", "subset": "wikitext-2-raw-v1", "split": "train" },
+            "pre_process_data_config": {
+                "strategy": "line-by-line",
+                "add_special_tokens": true,
+                "max_samples": 256,
+                "max_seq_len": 4096
+            }
+        }
+    ],
+    "passes": {
+        "q": { "type": "QuaRot" },
+        "g": {
+            "type": "GptqModel",
+            "bits": 4,
+            "sym": true,
+            "group_size": -1,
+            "lm_head": false,
+            "device": "cuda",
+            "data_config": "wikitext2_train_joined"
+        },
+        "cs": { "type": "CaptureSplitInfo", "num_splits": 4, "unique_embeds_lm_head_splits": true },
+        "mb": {
+            "type": "ModelBuilder",
+            "precision": "int4",
+            "int4_block_size": 32,
+            "int4_accuracy_level": 4,
+            "int4_op_types_to_quantize": [ "MatMul", "Gather" ]
+        },
+        "mq": {
+            "type": "MatMulNBitsToQDQ",
+            "use_int4": true,
+            "add_zero_point": true,
+            "nodes_to_exclude": [ "/lm_head/MatMul_Q4" ],
+            "save_as_external_data": true
+        },
+        "gs": {
+            "type": "GraphSurgeries",
+            "surgeries": [
+                { "surgeon": "RemoveRopeMultiCache" },
+                { "surgeon": "AttentionMaskToSequenceLengths" },
+                { "surgeon": "SimplifiedLayerNormToL2Norm" }
+            ],
+            "save_as_external_data": true
+        },
+        "sq": {
+            "type": "OnnxStaticQuantization",
+            "data_config": "wikitext2_train_act",
+            "activation_type": "uint16",
+            "precision": "uint8",
+            "calibration_providers": [ "CUDAExecutionProvider" ],
+            "quant_preprocess": true,
+            "op_types_to_exclude": [ "GatherBlockQuantized", "GroupQueryAttention", "MatMulNBits" ],
+            "save_as_external_data": true
+        },
+        "sp": { "type": "SplitModel" },
+        "st": { "type": "StaticLLM", "batch_size": 1, "context_length": 64 },
+        "cb": {
+            "type": "EPContextBinaryGenerator",
+            "provider_options": {
+                "htp_performance_mode": "burst",
+                "htp_graph_finalization_optimization_mode": "3",
+                "soc_model": "60"
+            },
+            "weight_sharing": true
+        },
+        "cp": { "type": "ComposeOnnxModels" }
+    },
+    "target": "qnn_system",
+    "log_severity_level": 1,
+    "output_dir": "models/qwen_2.5_7b_Instruct",
+    "cache_dir": "cache",
+    "no_artifacts": true
+}
-- 
2.46.0.windows.1

