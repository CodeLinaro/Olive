{
    "input_model": { "type": "HfModel", "model_path": "google/gemma-3-4b-it" , "model_attributes": {"head_dim": 256}},
    "systems": {
        "qnn_system": {
            "type": "PythonEnvironment",
            "python_environment_path": "/local/mnt2/workspace/kromero/olive/olive-venv/bin",
            "accelerators": [ { "execution_providers": [ "QNNExecutionProvider" ] } ]
        }
    },
    "data_configs": [
        {
            "name": "gemma_data_config",
            "user_script": "user_script.py",
            "load_dataset_config": { "type": "gemma_dataset", "model_id": "google/gemma-3-4b-it" }
        }
    ],
    "passes": {
        "q": { "type": "QuaRot" },
        "g": {
            "type": "GptqModel",
            "bits": 4,
            "sym": true,
            "group_size": -1,
            "lm_head": false,
            "device": "cuda",
            "data_config": "gemma_data_config"
        },
        "cs": { "type": "CaptureSplitInfo", "num_splits": 4, "unique_embeds_lm_head_splits": true },
        "mb": {
            "type": "ModelBuilder",
            "precision": "int4",
            "int4_block_size": 32,
            "int4_accuracy_level": 4,
            "int4_op_types_to_quantize": [ "MatMul", "Gather" ]
        },
        "mq": {
            "type": "MatMulNBitsToQDQ",
            "use_int4": true,
            "add_zero_point": true,
            "nodes_to_exclude": [ "/lm_head/MatMul_Q4" ],
            "save_as_external_data": true
        },
        "gs": {
            "type": "GraphSurgeries",
            "surgeries": [
                { "surgeon": "RemoveRopeMultiCache" },
                { "surgeon": "AttentionMaskToSequenceLengths" },
                { "surgeon": "SimplifiedLayerNormToL2Norm" }
            ],
            "save_as_external_data": true
        },
        "sq": {
            "type": "OnnxStaticQuantization",
            "data_config": "gemma_data_config",
            "activation_type": "uint16",
            "precision": "uint8",
            "calibration_providers": [ "CUDAExecutionProvider" ],
            "quant_preprocess": true,
            "op_types_to_exclude": [ "GatherBlockQuantized", "GroupQueryAttention", "MatMulNBits" ],
            "save_as_external_data": true
        },
        "sp": { "type": "SplitModel" },
        "st": { "type": "StaticLLM", "batch_size": 1, "context_length": 64 },
        "cb": {
            "type": "EPContextBinaryGenerator",
            "provider_options": {
                "htp_performance_mode": "burst",
                "htp_graph_finalization_optimization_mode": "3",
                "soc_model": "60"
            },
            "weight_sharing": true
        },
        "cp": { "type": "ComposeOnnxModels" }
    },
    "target": "qnn_system",
    "log_severity_level": 1,
    "output_dir": "models/gemma-3-4b-it",
    "cache_dir": "cache",
    "no_artifacts": true
}
